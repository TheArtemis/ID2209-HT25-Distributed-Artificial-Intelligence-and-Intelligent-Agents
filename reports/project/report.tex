\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{enumitem}

\geometry{margin=2.5cm}
\setlength{\parskip}{0.7em}
\setlength{\parindent}{0pt}
\setlength{\headheight}{14.49998pt}
\pagestyle{fancy}
\fancyhf{}
\rhead{DAIIA Assignment Report}
\lhead{Deflorian, Fragale, Skarbalius}
\cfoot{\thepage}

\titleformat{\section}{\large\bfseries\color{blue}}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\bfseries}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}[runin]{\bfseries}{\thesubsubsection}{0.5em}{}[.]

\title{\textbf{DAIIA Assignment Report}\\[0.5em]
\large Course: Distributed Artificial Intelligence and Intelligent Agents \\[0.3em]
Assignment: Final Project - Mars Colony: The Evolution of Trust}
\author{Lorenzo Deflorian, Riccardo Fragale, Juozas Skarbalius \\[0.3em]
KTH Royal Institute of Technology \\[0.3em]
}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Running Instructions}
\begin{itemize}[leftmargin=*]
  \item \textbf{How to run the program:} 
  \begin{itemize}
    \item Import the project files into GAMA platform
    \item Navigate to the folder \texttt{src/project/models}
    \item Open the file \texttt{MarsColony.gaml}
    \item From the GAMA interface, click the play button to start the simulation
    \item Use the simulation controls to adjust speed, pause, or reset as needed
  \end{itemize}
  \item \textbf{Expected inputs:} No manual inputs are required. The simulation runs automatically with predefined parameters that can be modified in the global section if desired (e.g., desired number of agents per type, map dimensions, learning parameters).
\end{itemize}

\section{General Overview}
\subsection*{Solution Summary}
This project presents a Mars Colony simulation with five agent roles (Engineer, Medic, Scavenger, Parasite, and Commander). Agents must survive in a resource-constrained environment by managing biological needs (oxygen, energy, health) and interacting with other agents.

The main focus is whether decentralized learning can reduce harmful interactions over time. Agents use a BDI (Belief-Desire-Intention) controller for survival behaviors and Q-Learning to guide social decisions (e.g., whether to trade with a specific partner).

\textbf{Key Features:}
\begin{itemize}
  \item Five distinct agent types with unique roles and behaviors
  \item Continuous simulation with supply shuttle system maintaining population
  \item BDI architecture for survival-oriented decision making
  \item Q-Learning with adaptive curiosity for trust-based parasite detection
  \item Facility replenishment system (greenhouse, oxygen generator) for survival alternatives
  \item FIPA communication for long-distance messaging (storm warnings)
  \item Real-time learning metrics visualization (trust, precision, recall)
  \item Behavioral metrics visualization (sociability, happiness, generosity)
  \item Survival metrics visualization (agent lifespan tracking)
\end{itemize}

\textbf{Current Implementation State:}
\begin{itemize}
  \item Death system is enabled with resource-based pressure enabling agent selection
  \item Oxygen and energy refill plans are enabled with slow facility replenishment (backup survival path)
  \item Trading remains the faster/better survival strategy than facilities alone
  \item Maximum colony size is capped at 60 agents
  \item Agents spawn at the habitat dome location
  \item Q-Learning uses exponential moving average (sociability = 0.35) for faster convergence
  \item Adaptive curiosity scales from 5\% to 50\% based on average trust
  \item Adaptive sociability recovery prevents learning stagnation
\end{itemize}

\section{Agent Types and Interactions}
\subsubsection*{3.1 Explanation}
The model includes five agent types with different roles, capabilities, and interaction rules. All agents share three core biological traits: \texttt{oxygen\_level}, \texttt{energy\_level}, and \texttt{health\_level}, which decrease over time and constrain survival. Each agent type follows specific rules that determine how it trades resources with other agents.

\textbf{Survival Mechanisms:} Agents can sustain themselves through two paths: (1) Trading with other agents for immediate resource boosts, or (2) Using facility replenishment systems (Greenhouse and OxygenGenerator) for slower but reliable resource restoration. Facilities provide +0.5 energy/cycle and +0.3 oxygen/cycle within their proximity radius, creating a survival baseline while trading provides faster growth opportunities.

\textbf{Agent Types:}
\begin{enumerate}
  \item \textbf{Engineer:} Repairs broken oxygen generators via BDI plan. Provides oxygen (+10) during trades.
    \item \textbf{Medic:} Heals other agents at the med-bay via a queue system. (Healing is performed at the Med-Bay, not through trading.)
  \item \textbf{Scavenger:} Ventures into wasteland to mine resources. Provides raw materials (converted to +20 energy) during trades.
  \item \textbf{Parasite:} Antagonist agent that steals resources without reciprocating. Uses \texttt{presented\_role} to disguise as other types.
  \item \textbf{Commander:} Broadcasts storm warnings via FIPA. Provides both oxygen (+10) and energy (+10) during trades.
\end{enumerate}

Each agent type has its own interaction logic. For example, Engineers repair generators, Medics heal at the med-bay, Scavengers mine resources, Parasites steal, and Commanders broadcast alerts.

\subsubsection*{3.2 Code}
The agent types are defined as species inheriting from the \texttt{Human} base species. Each agent initializes with its role and presented role:

\begin{verbatim}
species Engineer parent: Human {
    init {
        do add_desire(wander_desire);
        role <- "Engineer";
        presented_role <- "Engineer";
    }
    // ... Engineer-specific behaviors
}

species Parasite parent: Human {
    init {
        do add_desire(wander_desire);
        role <- "Parasite";
        presented_role <- one_of(["Engineer", "Medic", "Scavenger", "Commander"]);
    }
}
\end{verbatim}

The trading mechanism is implemented in the \texttt{attempt\_trade} action, which handles different behaviors based on agent types:

\begin{verbatim}
action attempt_trade(Human partner) {
    bool i_gave <- false;
    bool partner_gave <- false;
    
    if (!(self is Parasite)) {
        if (self is Scavenger and raw_amount > 0) {
            raw_amount <- raw_amount - 1;
            i_gave <- true;
            ask partner { energy_level <- min(max_energy_level, energy_level + 20.0); }
        }
        // ... other agent type behaviors
    }
    
    if (partner is Parasite) {
        partner_gave <- false;  // Parasites never give back
    }
    
    if (self is Parasite) {
        // Parasite performs harmful, one-sided interaction
        i_gave <- true;
        partner_gave <- false;
        // ... stealing logic
    }
    
    if (i_gave and partner_gave) { return 100.0; }  // Strong reward for mutual cooperation
    if (i_gave and !partner_gave) { 
        if (partner is Parasite) { return -80.0; }  // Victim strongly penalized for trusting parasite
        if (self is Parasite) { return -80.0; }     // Parasite strongly penalized for stealing
        return 20.0;  // Non-parasite helpfully gave, good reward for altruism
    }
    return 0.0;  // Neutral outcome for non-interaction
}
\end{verbatim}

\subsubsection*{3.3 Demonstration}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Input:} Simulation starts with all five agent types present in the habitat dome.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/map_Mars.png}
        \caption{Agents inside the map}
    \label{fig:agentsinmap}
\end{figure}
    \item \textbf{Interpretation:} The simulation starts with all five agent types present in the habitat dome. Agents are rendered with distinct visuals and begin moving immediately.
  
  \item \textbf{Input:} Agents engage in trading interactions within the habitat dome.
  \item \textbf{Screenshot:} \ref{fig:trading} \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/tradeHabitat.png}
    \caption{Trading between agents}
    \label{fig:trading}
\end{figure}
    \item \textbf{Interpretation:} As can be seen from \ref{fig:trading} trades follow the intended type-specific rules: Scavengers provide energy via raw resources, Engineers provide oxygen/repairs, Commanders provide oxygen+energy, and Parasites attempt one-sided interactions. Medic contributions occur through the Med-Bay queue (not via trade).
\end{enumerate}

\section{Environment and Places}
\subsubsection*{4.1 Explanation}
The environment is organized into distinct locations where agents move, interact, and perform activities. It includes a safe zone (habitat dome) and a hazard zone (wasteland), each with different effects on survival.

\textbf{Places:}
\begin{enumerate}
    \item \textbf{Habitat Dome:} Large safe zone (250x250 units) containing Greenhouse, Oxygen Generator, Med-Bay, and the two meeting places used for social interaction.
    \item \textbf{Common Area:} Meeting/trading area (circle) inside the dome.
    \item \textbf{Recreation Area:} Meeting/trading area (circle) inside the dome.
    \item \textbf{Wasteland:} Danger zone (100x100 units) where Scavengers mine. Oxygen depletes 1.2x faster, and random dust storms increase risk.
    \item \textbf{Med-Bay:} Facility within the dome where injured agents queue for treatment. The queue status is shown visually (orange when patients are waiting).
  \item \textbf{Landing Pad:} Location where agents retire and despawn.
  \item \textbf{Rock Mine:} Location in wasteland where Scavengers collect raw materials.
\end{enumerate}

\subsubsection*{4.2 Code}
The places are defined as species with specific geometries and behaviors:

\begin{verbatim}
species HabitatDome {
    geometry shape <- rectangle(250, 250);
    Greenhouse greenhouse;
    OxygenGenerator oxygen_generator;
    MedBay med_bay;
    CommonArea common_area;
    RecreationArea recreation_area;
    
    init {
        location <- point(200, 200);
        shape <- shape at_location location;
        // Create and position facilities
        create Greenhouse number: 1 returns: greenhouses;
        greenhouse <- greenhouses[0];
        // ... initialize other facilities
    }
}

species Wasteland {
    geometry shape <- rectangle(100, 100);
    bool dust_storm <- false;
    int storm_timer <- 0;
    
    reflex manage_storm {
        if (dust_storm) {
            storm_timer <- storm_timer - 1;
            if (storm_timer <= 0) {
                dust_storm <- false;
            }
        } else {
            if (flip(0.01)) {
                dust_storm <- true;
                storm_timer <- 15;
            }
        }
    }
}

species Greenhouse {
    point location;
    float replenish_rate <- 0.5;  // Energy restored per cycle
    
    reflex replenish_energy {
        list<Human> nearby_agents <- Human where 
            (each.location distance_to location <= facility_proximity);
        loop h over: nearby_agents {
            ask h {
                if (energy_level < max_energy_level) {
                    energy_level <- min(max_energy_level, 
                                       energy_level + myself.replenish_rate);
                }
            }
        }
    }
}

species OxygenGenerator {
    point location;
    bool is_broken <- false;
    float replenish_rate <- 0.3;  // Oxygen restored per cycle
    
    reflex replenish_oxygen when: not is_broken {
        list<Human> nearby_agents <- Human where 
            (each.location distance_to location <= facility_proximity);
        loop h over: nearby_agents {
            ask h {
                if (oxygen_level < max_oxygen_level) {
                    oxygen_level <- min(max_oxygen_level, 
                                       oxygen_level + myself.replenish_rate);
                }
            }
        }
    }
}
\end{verbatim}

Oxygen depletion rates differ based on location:

\begin{verbatim}
reflex update_oxygen{
    if (habitat_dome.shape covers location) {
        oxygen_level <- max(0, oxygen_level - oxygen_decrease_rate);
    } else {
        float decrease <- oxygen_decrease_rate * oxygen_decrease_factor_in_wasteland;
        if (wasteland.dust_storm and (wasteland.shape covers location)) {
             decrease <- decrease * 2.0;
        }
        oxygen_level <- max(0, oxygen_level - decrease);
    }
}
\end{verbatim}

\subsubsection*{4.3 Demonstration}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Input:} Simulation displays the full map with all places visible.
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.37\textwidth]{imgs/map_Mars.png}
    \caption{Mars colony map}
    \label{fig:marsmap}
\end{figure}
    \item \textbf{Interpretation:} All places are initialized and positioned as expected. The habitat dome functions as the primary safe zone, while the wasteland introduces harsher environmental conditions.
  
  \item \textbf{Input:} A dust storm occurs in the wasteland.
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.55\textwidth]{imgs/Storm_Wasteland.png}
    \caption{Storm in the Wasteland}
    \label{fig:stormwasteland}
\end{figure}
    \item \textbf{Interpretation:} During a dust storm, the wasteland changes color to provide immediate visual feedback.
\end{enumerate}

\section{Continuous Running System}
\subsubsection*{5.1 Explanation}
The simulation runs continuously by maintaining a target population through a supply shuttle system. When agents retire (after 2000 cycles) or die (when health reaches 0), new agents are spawned to maintain desired population levels. The system maintains at least 50 agents total across all types, with a maximum cap of 60 agents. Death events are tracked for survival metrics, including average lifespan of deceased agents.

The supply shuttle operates in "deficit mode," spawning new agents when any agent type falls below its desired count:
\begin{itemize}
  \item Engineers: 16 desired
  \item Medics: 10 desired
  \item Scavengers: 8 desired
  \item Parasites: 12 desired
  \item Commanders: 4 desired
\end{itemize}

Additionally, the supply shuttle aggregates Q-tables and trust memory from all survivors to calculate averages (preparing for potential knowledge inheritance by new agents).

\subsubsection*{5.2 Code}
The supply shuttle reflex checks population counts and spawns new agents:

\begin{verbatim}
reflex supply_shuttle when: enable_supply_shuttle {
    int total_colonists <- length(list(Engineer) + list(Medic) + 
                                  list(Scavenger) + list(Parasite) + 
                                  list(Commander));
    
    bool deficit_mode <- (current_number_of_engineers < desired_number_of_engineers) or
                          (current_number_of_medics < desired_number_of_medics) or
                          (current_number_of_scavengers < desired_number_of_scavengers) or
                          (current_number_of_parasites < desired_number_of_parasites) or
                          (current_number_of_commanders < desired_number_of_commanders);
    
    if (deficit_mode) {
        int max_to_spawn <- max_colony_size - total_colonists;
        
        // Spawn engineers if needed
        if (delta_engineers > 0 and max_to_spawn > 0) {
            int to_spawn <- min(delta_engineers, max_to_spawn);
            create Engineer number: to_spawn returns: new_engineers;
            ask new_engineers { 
                location <- habitat_dome.location; 
                oxygen_level <- max_oxygen_level; 
                energy_level <- max_energy_level; 
            }
            engineers <- engineers + new_engineers;
            current_number_of_engineers <- current_number_of_engineers + to_spawn;
            max_to_spawn <- max_to_spawn - to_spawn;
        }
        // ... similar logic for other agent types
    }
}
\end{verbatim}

Retirement is handled through BDI:

\begin{verbatim}
reflex update_eta {
    if (not enable_retirement) { return; }
    eta <- eta + eta_increment;
    if (eta >= retirement_age) { 
        do add_belief(should_retire_belief); 
    }
}

plan do_retire intention: retire_desire {
    state <- "retiring";
    do goto target: landing_pad.location speed: movement_speed;
    
    if ((location distance_to landing_pad.location) <= facility_proximity) {
        do die_and_update_counter;
    }
}
\end{verbatim}

\subsubsection*{5.3 Demonstration}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Input:} Simulation runs for an extended period, allowing agents to reach retirement age (2000 cycles).
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/agents_going_to_landing_pad.png}
    \caption{Agents going to the landing pad after retirement}
    \label{fig:agentsleaving}
  \end{figure}
    \item \textbf{Interpretation:} The retirement system tracks agent age and triggers retirement when the threshold is reached. Retiring agents move to the landing pad and despawn.
  
  \item \textbf{Input:} Agents retire, causing population counts to drop below desired levels.
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/supply_shuttle_arrives.png}
    \caption{Supply shuttle arriving with new agents}
    \label{fig:supplyshuttle}
  \end{figure}
    \item \textbf{Interpretation:} When population counts fall below the targets, the supply shuttle spawns new agents to restore the desired levels. This keeps the simulation running without manual intervention.
\end{enumerate}

\section{FIPA Communication}
\subsubsection*{6.1 Explanation}
FIPA (Foundation for Intelligent Physical Agents) messaging is used for long-distance coordination. In this simulation, Commanders send storm warnings to agents in the wasteland when a dust storm is detected. The goal is to model asynchronous communication that can influence behavior at a distance.

When a dust storm occurs in the wasteland, Commanders detect it and broadcast a "Return to Base" message using the FIPA propose protocol. Agents receiving this message add a \texttt{storm\_warning\_belief} to their BDI system, which triggers the highest-priority desire (\texttt{escape\_storm}) to return to the safe habitat dome.

\subsubsection*{6.2 Code}
Commanders monitor for storms and send FIPA messages:

\begin{verbatim}
species Commander parent: Human {
    reflex check_storm {
        if (wasteland.dust_storm) {
            list<Human> agents_in_wasteland <- Human where 
                (wasteland.shape covers each.location);
            if (!empty(agents_in_wasteland)) {
                do start_conversation to: agents_in_wasteland 
                   protocol: "fipa-propose"
                   performative: "propose" 
                   contents: ["Return to Base"];
            }
        }
    }
}
\end{verbatim}

Agents receive and process FIPA messages:

\begin{verbatim}
reflex receive_message when: !empty(mailbox) {
    message msg <- first(mailbox);
    mailbox <- mailbox - msg;
    string msg_contents <- string(msg.contents);
    if (msg_contents contains "Return to Base") {
        if (not has_belief(storm_warning_belief)) { 
            do add_belief(storm_warning_belief); 
        }
    }
}
\end{verbatim}

The BDI system processes the belief:

\begin{verbatim}
rule belief: storm_warning_belief new_desire: escape_storm_desire strength: 200.0;

plan escape_storm intention: escape_storm_desire {
    state <- "escaping_storm";
    do goto target: habitat_dome.location speed: movement_speed;
    
    if (habitat_dome.shape covers location) {
        do remove_belief(storm_warning_belief);
        do remove_intention(escape_storm_desire, true);
        state <- "idle";
    }
}
\end{verbatim}

\subsubsection*{6.3 Demonstration}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Input:} A dust storm occurs in the wasteland while agents (particularly Scavengers) are present there.
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/return_to_base.png}
    \caption{Agents receiving FIPA "Return to Base" messages during storm}
    \label{fig:returntobase}
  \end{figure}
    \item \textbf{Interpretation:} Commanders detect the storm and broadcast FIPA messages to agents in the wasteland. Receiving agents process the messages and add the storm warning belief.
  
  \item \textbf{Input:} Agents receive storm warning messages and begin moving toward the habitat dome.
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/agents_returning_to_base.png}
    \caption{Agents returning to base after receiving storm warning}
    \label{fig:agentsreturning}
  \end{figure}
    \item \textbf{Interpretation:} The warning message changes agent priorities: agents with a storm warning prioritize escape (strength 200.0) and move back to the habitat dome.
\end{enumerate}

\section{Charts and Monitoring}
\subsubsection*{7.1 Explanation}
The simulation provides real-time monitoring of learning metrics through charts and global values. The main chart tracks trust and parasite-detection metrics over time.

\textbf{Global Values and Metrics:}
\begin{itemize}
  \item \texttt{avg\_trust\_to\_parasites}: Average trust value agents have toward parasites
  \item \texttt{avg\_trust\_to\_non\_parasites}: Average trust value agents have toward cooperative agents
  \item \texttt{precision}: True positives / (True positives + False positives) for parasite detection
  \item \texttt{recall}: True positives / (True positives + False negatives) for parasite detection
  \item \texttt{total\_trades}: Counter of total trade interactions
    \item \texttt{avg\_sociability}: Average learning-rate parameter across agents (scaled to 0-100 range)
  \item \texttt{avg\_happiness}: Average accumulated reward from social interactions
  \item \texttt{avg\_generosity}: Average giving/receiving balance tracked during trades
  \item \texttt{avg\_living\_agent\_age}: Average ETA (age) of currently living agents
  \item \texttt{avg\_dead\_agent\_lifespan}: Average lifespan of deceased agents
  \item \texttt{total\_deaths}: Total number of agents that have died
\end{itemize}

The expectation is that trust toward parasites decreases over time while trust toward non-parasites remains positive, and that precision/recall improve as agents accumulate interactions.

\textbf{Interesting Conclusion:}
In repeated runs, negative outcomes with parasites reduce trust and increase avoidance. This functions like a decentralized reputation effect: agents do not need a centralized authority to change behavior, but instead adapt based on their own interaction history.

\subsubsection*{7.2 Code}
Learning metrics are updated each cycle:

\begin{verbatim}
reflex update_learning_metrics {
    int total_agents <- length(list(Engineer) + list(Medic) + 
                               list(Scavenger) + list(Parasite) + 
                               list(Commander));
    
    map<string, bool> is_parasite_by_id <- map([]);
    loop h over: (list(Engineer) + list(Medic) + list(Scavenger) + 
                  list(Parasite) + list(Commander)) {
        is_parasite_by_id["id:" + h.name] <- (h is Parasite);
    }
    
    float sum_par <- 0.0; int cnt_par <- 0;
    float sum_non <- 0.0; int cnt_non <- 0;
    int tp0 <- 0; int fp0 <- 0; int fn0 <- 0; int tn0 <- 0;

    float sum_soc <- 0.0;
    float sum_hap <- 0.0;
    float sum_gen <- 0.0;
    
    loop h over: (list(Engineer) + list(Medic) + list(Scavenger) + 
                  list(Parasite) + list(Commander)) {
        sum_soc <- sum_soc + h.sociability;
        sum_hap <- sum_hap + h.happiness;
        sum_gen <- sum_gen + h.generosity;

        loop k over: keys(h.trust_memory) {
            float v <- h.trust_memory[k];
            bool gt_par <- ((is_parasite_by_id contains_key k) ? 
                           is_parasite_by_id[k] : false);
            if (gt_par) { 
                sum_par <- sum_par + v; 
                cnt_par <- cnt_par + 1; 
            } else { 
                sum_non <- sum_non + v; 
                cnt_non <- cnt_non + 1; 
            }
            
            bool pred_par <- v < 0.0;
            if (pred_par and gt_par) { tp0 <- tp0 + 1; }
            else if (pred_par and not gt_par) { fp0 <- fp0 + 1; }
            else if ((not pred_par) and gt_par) { fn0 <- fn0 + 1; }
            else { tn0 <- tn0 + 1; }
        }
    }
    
    avg_trust_to_parasites <- (cnt_par > 0 ? sum_par / float(cnt_par) : 0.0);
    avg_trust_to_non_parasites <- (cnt_non > 0 ? sum_non / float(cnt_non) : 0.0);
    precision <- ((tp0 + fp0) > 0 ? float(tp0) / float(tp0 + fp0) : 0.0);
    recall <- ((tp0 + fn0) > 0 ? float(tp0) / float(tp0 + fn0) : 0.0);

    avg_sociability <- (total_agents > 0 ? sum_soc / float(total_agents) : 0.0);
    avg_happiness <- (total_agents > 0 ? sum_hap / float(total_agents) : 0.0);
    avg_generosity <- (total_agents > 0 ? sum_gen / float(total_agents) : 0.0);
}
\end{verbatim}

The chart is defined in the experiment:

\begin{verbatim}
display LearningMetrics {
    chart "Avg Trust & Detection" type: series {
        data 'Avg trust (parasites)' value: avg_trust_to_parasites;
        data 'Avg trust (non-parasites)' value: avg_trust_to_non_parasites;
        data 'Precision' value: precision;
        data 'Recall' value: recall;
    }
}

display BehavioralMetrics {
    chart "Sociability, Happiness \& Generosity" type: series {
        data 'Avg Sociability' value: avg_sociability color: \#blue;
        data 'Avg Happiness' value: avg_happiness color: \#green;
        data 'Avg Generosity' value: avg_generosity color: \#orange;
    }
}

display SurvivalMetrics {
    chart "Agent Lifespan \& Survival" type: series {
        data 'Avg Living Agent Age' value: avg_living_agent_age color: rgb(0, 102, 204);
        data 'Avg Dead Agent Lifespan' value: avg_dead_agent_lifespan color: rgb(204, 0, 0);
    }
}
\end{verbatim}

\subsubsection*{7.3 Demonstration}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Input:} Simulation runs for an extended period to allow learning to occur.
    \item \textbf{Interpretation of the figure below:} The learning metrics chart shows how trust and detection accuracy change over time. Precision and recall follow the standard machine learning definitions.
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/learningMetrics.png}
    \caption{Learning metrics}
    \label{fig:learningmetrics}
\end{figure}
  \item \textbf{Input:} Agent state inspector shows trust memory values for different agents.
  \item \textbf{Screenshot:} \begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/behaviouralMetrics.png}
    \caption{Behavioural metrics map}
    \label{fig:behaviouralmetrics}
\end{figure}
    \item \textbf{Interpretation:} Trust memory maps separate parasites (negative trust) from cooperative agents (positive trust), reflecting the cumulative outcomes of trade interactions.
\end{enumerate}

\section{Challenge 1: BDI Agents}
\subsubsection*{8.1 Explanation}
The simulation implements a full BDI (Belief-Desire-Intention) architecture using GAMA's Simple BDI control. This architecture allows agents to make decisions based on their beliefs about the world, desires (goals) they want to achieve, and intentions (plans) to fulfill those desires.

\textbf{Beliefs (Sensors):} Agents maintain beliefs about their internal state and external environment:
\begin{itemize}
  \item \texttt{suffocating\_belief}: Triggered when oxygen level $<$ 20\%
  \item \texttt{starving\_belief}: Triggered when energy level $<$ 20\%
  \item \texttt{injured\_belief}: Triggered when health level $<$ 50\%
  \item \texttt{should\_retire\_belief}: Triggered when ETA $\geq$ 2000 cycles
  \item \texttt{storm\_warning\_belief}: Triggered by FIPA messages from Commander
  \item \texttt{generator\_broken\_belief}: Engineer-specific, triggered when oxygen generator is broken
  \item \texttt{patients\_waiting\_belief}: Medic-specific, triggered when med-bay queue has patients
  \item \texttt{mission\_time\_belief}: Scavenger-specific, triggered with 20\% probability per cycle
  \item \texttt{want\_to\_trade\_belief}: Triggered when agent assesses trading motivation (probability: 0.5 + curiosity)
\end{itemize}

\textbf{Desires (Goals):} Desires are ranked by rule strengths, determining priority:
\begin{itemize}
  \item \texttt{retire} (500.0) - Highest priority
  \item \texttt{escape\_storm} (200.0) - High priority
  \item \texttt{has\_oxygen} (100.0) - High priority
  \item \texttt{has\_energy} (25.0) - Medium priority
  \item \texttt{be\_healthy} (12.0) - Medium-low priority
  \item \texttt{seek\_trading\_area} (8.0) - Medium-low priority
  \item \texttt{fix\_generator} (9.0) - Engineer only
  \item \texttt{heal\_patients} (7.0) - Medic only
  \item \texttt{mine\_resources} (5.0) - Scavenger only
  \item \texttt{wander} (default) - Lowest priority
\end{itemize}

\textbf{Intentions (Plans):} Plans define the actions agents take to fulfill desires. Each plan is associated with a specific desire and executes until the desire is satisfied or the plan is completed.

\subsubsection*{8.2 Code}
Beliefs are managed through perception reflexes:

\begin{verbatim}
reflex perception {
    if (oxygen_level < oxygen_level_threshold) {
        if (not has_belief(suffocating_belief)) { 
            do add_belief(suffocating_belief); 
        }
    } else {
        if (has_belief(suffocating_belief)) { 
            do remove_belief(suffocating_belief); 
        }
    }
    // Similar logic for starving_belief and injured_belief
}
\end{verbatim}

Desires are created through rules that link beliefs to desires:

\begin{verbatim}
rule belief: should_retire_belief new_desire: retire_desire strength: 500.0;
rule belief: storm_warning_belief new_desire: escape_storm_desire strength: 200.0;
rule belief: suffocating_belief new_desire: has_oxygen_desire strength: 100.0;
rule belief: starving_belief new_desire: has_energy_desire strength: 25.0;
rule belief: injured_belief new_desire: be_healthy_desire strength: 12.0;
rule belief: want_to_trade_belief new_desire: seek_trading_area_desire strength: 8.0;
\end{verbatim}

Plans execute intentions:

\begin{verbatim}
plan get_health intention: be_healthy_desire 
      finished_when: health_level >= max_health_level {
    if ((location distance_to habitat_dome.med_bay.location) <= facility_proximity) {
        state <- "waiting_at_med_bay";
        ask habitat_dome.med_bay { do add_to_queue(myself); }
    } else {
        state <- "going_to_med_bay";
        do goto target: habitat_dome.med_bay.location speed: movement_speed;
    }
    
    if (health_level >= max_health_level) {
        ask habitat_dome.med_bay { do remove_from_queue(myself); }
        do remove_belief(injured_belief);
    }
}

plan get_oxygen intention: has_oxygen_desire 
      finished_when: oxygen_level >= max_oxygen_level * 0.8 {
    if ((location distance_to habitat_dome.oxygen_generator.location) <= facility_proximity) {
        state <- "at_oxygen_generator";
        // Facility reflex handles replenishment automatically
    } else {
        state <- "going_to_oxygen";
        do goto target: habitat_dome.oxygen_generator.location speed: movement_speed;
    }
}

plan get_energy intention: has_energy_desire 
      finished_when: energy_level >= max_energy_level * 0.8 {
    if ((location distance_to habitat_dome.greenhouse.location) <= facility_proximity) {
        state <- "at_greenhouse";
        // Facility reflex handles replenishment automatically
    } else {
        state <- "going_to_greenhouse";
        do goto target: habitat_dome.greenhouse.location speed: movement_speed;
    }
}
\end{verbatim}

Engineer-specific BDI behavior:

\begin{verbatim}
species Engineer parent: Human {
    predicate generator_broken_belief <- new_predicate("generator_broken");
    predicate fix_generator_desire <- new_predicate("fix_generator");
    
    reflex check_generator {
        if (habitat_dome.oxygen_generator.is_broken) {
            if (not has_belief(generator_broken_belief)) { 
                do add_belief(generator_broken_belief); 
            }
        }
    }
    
    rule belief: generator_broken_belief new_desire: fix_generator_desire strength: 9.0;
    
    plan fix_generator intention: fix_generator_desire {
        if ((location distance_to habitat_dome.oxygen_generator.location) <= facility_proximity) {
            habitat_dome.oxygen_generator.is_broken <- false;
            do remove_belief(generator_broken_belief);
            do remove_intention(fix_generator_desire, true);
            state <- "idle";
        } else {
            state <- "going_to_oxygen_generator";
            do goto target: habitat_dome.oxygen_generator.location speed: movement_speed;
        }
    }
}
\end{verbatim}

\subsubsection*{8.3 Demonstration}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Input:} An agent's oxygen level drops below 20\% threshold.
  \item \textbf{Screenshot:} 
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.55\textwidth]{imgs/OxygenLevel.png}
    \caption{Oxygen levels down}
    \label{fig:oxygenlevel}
  \end{figure}
    \item \textbf{Interpretation:} When oxygen falls below the threshold, the perception reflex adds \texttt{suffocating\_belief}. The BDI system creates \texttt{has\_oxygen\_desire} with high priority (100.0), and the agent executes \texttt{get\_oxygen} to reach the oxygen generator. Once within range, the generator replenishes oxygen at 0.3 per cycle.
  
  \item \textbf{Input:} An agent's health drops below 50\% threshold.
  \item \textbf{Screenshot:} 
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/healthLevel.png}
    \caption{Health level below threshold}
    \label{fig:healthLow}
  \end{figure}
    \item \textbf{Interpretation:} When health drops below the threshold, the agent adds the \texttt{injured\_belief}, creates the \texttt{be\_healthy\_desire}, and executes the \texttt{get\_health} plan. The agent navigates to the med-bay and queues for treatment.
  
  \item \textbf{Input:} The oxygen generator breaks (5\% probability per cycle).
  \item \textbf{Screenshot:} 
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.25\textwidth]{imgs/oxygen_generator.png}
    \caption{Oxygen generator broken}
    \label{fig:oxygengenerator}
  \end{figure}
    \item \textbf{Interpretation:} Engineers detect the broken generator via \texttt{check\_generator}, add \texttt{generator\_broken\_belief}, and execute the repair plan. Only Engineers respond to generator failures.
  
  \item \textbf{Input:} Multiple agents have different priority desires simultaneously (e.g., one suffocating, one injured, one with storm warning).
  \item \textbf{Screenshot:} 
   \begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/differentStates.png}
    \caption{Different states}
    \label{fig:diffStates}
  \end{figure}
    \item \textbf{Interpretation:} Desire selection follows rule strengths. For example, agents with a storm warning (200.0) prioritize escape over other tasks, while suffocating agents (100.0) prioritize oxygen above low-priority goals such as healing (12.0).
\end{enumerate}

\section{Challenge 2: Reinforcement Learning}
\subsubsection*{9.1 Explanation}
The simulation implements Q-Learning, a reinforcement learning algorithm, to enable agents to learn which other agents are trustworthy through experience. 
Agents learn to identify parasite agents by tracking the outcomes of trade interactions (e.g., trades of energy, health, etc.) and updating Q-values accordingly.
\\ \\
There are two fixed-coordinate areas where agents can trade. When agents sense a need for a resource replenishment (e.g., low energy, low health), they move to a trading area and randomly select another agent present to attempt a trade. The trade interaction results in rewards based on the actions of both agents (mutual cooperation, altruism, theft, etc.). Agents update their Q-values based on these rewards, allowing them to learn trustworthiness over time.
\textbf{Q-Learning Implementation:}
\begin{itemize}
  \item \textbf{State:} Agent ID ("id:agent\_name") - Each agent tracks trust per individual, not per type
  \item \textbf{Actions:} TRADE or IGNORE (selected via $\epsilon$-greedy: 20\% random exploration, 80\% exploitation)
  \item \textbf{Rewards:}
    \begin{itemize}
      \item Mutual cooperation (both give): +100.0
      \item Altruism (non-parasite gives goods without receiving anything back): +20.0
      \item Victim trusts parasite: -80.0
      \item Parasite steals: -80.0
      \item IGNORE (avoid risky interaction): +2.0
      \item Failed/no-interaction: 0.0
    \end{itemize}
  \item \textbf{Q-Value Update:} $Q(s,a) = Q(s,a) + \alpha(reward - Q(s,a))$ where $\alpha = 0.35$ (learning rate/sociability)
  \item \textbf{Trust Calculation:} $trust = \frac{Q[TRADE] - Q[IGNORE]}{100.0}$, clamped to $[-1.0, 1.0]$
  \item \textbf{Detection:} Trust $< 0$ indicates predicted parasite
  \item \textbf{Adaptive Learning:} Sociability gradually recovers if it drops below 0.15 to prevent learning stagnation
\end{itemize}

\textbf{Learning Evolution:}
Initially, agents interact randomly (curiosity = 20\%). Parasites attempt to exploit others, gaining -80 rewards when caught stealing. Over time, Q-values for parasite interactions decrease due to accumulated negative rewards. Trust values drop toward -0.8 to -1.0 for parasites. Agents learn to prefer IGNORE actions (reward +2) over trading with distrusted partners. Adaptive curiosity increases exploration when agents are struggling (low average trust), forcing them out of local optima. The learning rate (sociability = 0.35) enables fast convergence. The learning metrics track average trust to parasites vs non-parasites, plus precision/recall for parasite detection.

\subsubsection*{9.2 Code}
Q-Learning state and action selection:

\begin{verbatim}
map<string, list<float>> Q <- map([]);
map<string, float> trust_memory <- map([]);

float sociability <- 0.35;  // Learning rate (increased for faster convergence)
float curiosity <- 0.20;    // Exploration rate

action ensure_state_exists(string s) {
    if (!(Q contains_key s)) { 
        Q[s] <- [0.0, 0.0];  // [Q(TRADE), Q(IGNORE)]
    }
    if (!(trust_memory contains_key s)) { 
        trust_memory[s] <- 0.0; 
    }
}

action choose_action(string s) {
    // Epsilon-greedy: explore or exploit
    if (flip(curiosity)) { 
        return (flip(0.5) ? "TRADE" : "IGNORE"); 
    }
    list<float> qs <- Q[s];
    return (qs[0] >= qs[1] ? "TRADE" : "IGNORE");
}
\end{verbatim}

Q-value update with proper exponential moving average:

\begin{verbatim}
action update_q(string s, string a, float r) {
    do ensure_state_exists(s);
    int idx <- (a = "TRADE" ? 0 : 1);
    
    list<float> qs <- Q[s];
    float old <- qs[idx];
    // Exponential moving average: converges to average reward
    float updated <- old + sociability * (r - old);
    qs[idx] <- updated;
    Q[s] <- qs;
    
    // Calculate trust and downscale trust (from -100 and +100 to -1.0 to +1.0)
    float pref <- Q[s][0] - Q[s][1];
    trust_memory[s] <- max(-1.0, min(1.0, pref / 100.0));  // Normalize by max expected difference
}

reflex adapt_learning_parameters when: cycle mod 100 = 0 and length(keys(trust_memory)) > 0 {
    // Calculate average trust from recent interactions
    float total_trust <- 0.0;
    int count <- 0;
    loop t over: values(trust_memory) {
        total_trust <- total_trust + t;
        count <- count + 1;
    }
    float avg_trust_value <- total_trust / float(count);
    
    // Adapt curiosity: increase when learning is poor, decrease when good
    if (avg_trust_value < -0.2) {
        curiosity <- min(0.50, base_curiosity + 0.15);
    } else if (avg_trust_value < 0.0) {
        curiosity <- min(0.45, base_curiosity + 0.10);
    } else if (avg_trust_value < 0.3) {
        curiosity <- base_curiosity;
    } else if (avg_trust_value < 0.6) {
        curiosity <- max(0.10, base_curiosity - 0.05);
    } else {
        curiosity <- max(0.05, base_curiosity - 0.10);
    }
    
    // Gradually recover sociability if it has dropped too low
    if (sociability < 0.15) {
        sociability <- sociability + 0.01;
    }
    trust_memory[s] <- max(-1.0, min(1.0, pref / 100.0));
}
\end{verbatim}

Adaptive learning parameters:

\begin{verbatim}
reflex adapt_learning_parameters when: cycle mod 100 = 0 and 
    length(keys(trust_memory)) > 0 {
    // Calculate average trust
    float total_trust <- 0.0;
    loop t over: values(trust_memory) { 
        total_trust <- total_trust + t; 
    }
    float avg_trust <- total_trust / float(length(values(trust_memory)));
    
    // Adapt curiosity: explore more when struggling, exploit when successful
    if (avg_trust < -0.2) {
        curiosity <- min(0.50, base_curiosity + 0.15);
    } else if (avg_trust < 0.0) {
        curiosity <- min(0.45, base_curiosity + 0.10);
    } else if (avg_trust < 0.3) {
        curiosity <- base_curiosity;
    } else if (avg_trust < 0.6) {
        curiosity <- max(0.10, base_curiosity - 0.05);
    } else {
        curiosity <- max(0.05, base_curiosity - 0.10);
    }
    
    // Recover sociability if stuck in local optima
    if (sociability < 0.15) {
        sociability <- sociability + 0.01;
    }
}
\end{verbatim}

Trade interaction with learning:

\begin{verbatim}
reflex assess_trading_motivation when:
    trade_cooldown = 0
    and (habitat_dome.shape covers location)
    and not has_belief(storm_warning_belief)
    and not has_belief(want_to_trade_belief)
{
    if (flip(0.5 + curiosity)) {
        do add_belief(want_to_trade_belief);
    }
}

reflex learn_and_trade when:
    trade_cooldown = 0
    and (habitat_dome.common_area.shape covers location or habitat_dome.recreation_area.shape covers location)
    and not has_belief(storm_warning_belief)
{
    list<Human> all_humans <- list(Engineer) + list(Medic) + 
                              list(Scavenger) + list(Parasite) + 
                              list(Commander);
    list<Human> nearby <- [];
    
    loop h over: all_humans {
        if (h != self) {
            float dist <- h.location distance_to location;
            if (dist <= meet_distance) {
                nearby <- nearby + [h];
            }
        }
    }
    
    if (empty(nearby)) { return; }
    
    Human partner <- one_of(nearby);
    string s <- "id:" + partner.name;
    do ensure_state_exists(s);
    
    string a <- choose_action(s);
    if (a = "IGNORE") {
        do update_q(s, a, 2.0);
        trade_cooldown <- trade_cooldown_max;
        return;
    }
    
    float reward <- attempt_trade(partner);
    do update_q(s, a, reward);
    happiness <- happiness + reward;
    total_trades <- total_trades + 1;
    trade_cooldown <- trade_cooldown_max;  // Cooldown is 5 cycles
}
\end{verbatim}

\subsubsection*{9.3 Demonstration}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Input:} Simulation starts with agents having empty Q-tables and trust memory (Ref. \ref{fig:initialTrust}). Agents begin trading with each other. 
  \item \textbf{Screenshot:}
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/q1.png}
    \caption{Initial Q values}
    \label{fig:initialTrust}
  \end{figure}
    \item \textbf{Interpretation:} Initially, Q-values are zero, so agents explore trade decisions frequently under the $\epsilon$-greedy policy.
  
  \item \textbf{Input:} Simulation runs for several hundred cycles, allowing multiple trade interactions with the same agents.
  \item \textbf{Screenshot:}
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/q2.png}
    \caption{Q values after 10 cycles}
    \label{fig:after10cycles}
  \end{figure}
    \item \textbf{Interpretation:} The Q-table accumulates positive and negative values based on trade outcomes (Fig. \ref{fig:after10cycles}). After repeated interactions, agents start separating parasites from cooperative partners and increasingly exploit learned preferences (80\% greedy).

  \item \textbf{Input:} Simulation runs for an extended period (1000+ cycles) with learning metrics being tracked.
  \item \textbf{Screenshot:}
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/q3.png}
    \caption{Avg. trust after extended learning}
    \label{fig:avgtrust}
  \end{figure}
    \item \textbf{Interpretation:} After extended learning, average trust becomes strongly negative for parasites and remains positive for cooperative agents (Fig. \ref{fig:avgtrust}). Adaptive curiosity settles around 10--15\% exploration once average trust is high.

  \item \textbf{Input:} Simulation runs for an extended period (1000+ cycles) with average agent lifespan being tracked.
  \item \textbf{Screenshot:}
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/q4.png}
    \caption{Avg. lifespan of dead and alive agents}
    \label{fig:lifespans}
  \end{figure}
    \item \textbf{Interpretation:} As shown in \ref{fig:lifespans}, the average lifespan of living agents increases over time, while the average lifespan of deceased agents stabilizes. This is consistent with fewer damaging interactions as agents learn to avoid risky partners.
\end{enumerate}

\section{Final Remarks}
This project shows how individual learning can shape system-level behavior in a distributed multi-agent setting. The Mars Colony simulation combines BDI architecture for survival behaviors with Q-Learning for social decisions, allowing agents to adapt their interactions based on experience.

\textbf{Key Achievements:}
\begin{itemize}
    \item Implemented a full BDI architecture with multiple belief types, prioritized desires, and executable plans
  \item Implemented Q-Learning with state-action-reward framework for trust-based decision making
  \item Created a continuous simulation system that maintains population through supply shuttles
  \item Integrated FIPA communication for long-distance messaging
  \item Demonstrated learning evolution through metrics showing improved parasite detection over time
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
  \item Learning is individual - agents don't directly share knowledge (though they could observe patterns)
  \item Parasites use simple disguise mechanism - could be enhanced with more sophisticated deception
  \item The \texttt{patience} parameter (discount factor) is defined but not used in the learning algorithm
\end{itemize}

\textbf{Potential Improvements:}
\begin{itemize}
  \item Apply Q-table inheritance so new agents start with averaged knowledge from survivors
  \item Implement knowledge sharing mechanisms (e.g., agents can share trust information)
  \item Implement the \texttt{patience} parameter (discount factor) for temporal discounting of future rewards
  \item Add more sophisticated parasite strategies (e.g., temporary cooperation, selective stealing)
  \item Implement reputation systems where agents can observe others' interactions
  \item Add more complex reward structures (e.g., reputation-based rewards, long-term relationship benefits)
  \item Enhance visualization with additional charts (population over time, trade success rates, death rates, etc.)
\end{itemize}

\textbf{Conclusion:}
Over time, agents learn to avoid parasites through reinforcement learning. Without centralized control, behavior shifts as agents adapt to negative experiences, which provides an intuition for how trust and avoidance can emerge in multi-agent systems.

\end{document}
